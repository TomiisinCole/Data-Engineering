## About The Project

A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming application. Sparkify has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the application, as well as a directory with JSON metadata on the songs in their application.

They'd like a data engineer to build an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to. The role of this project is to create a data warehouse on cloud (AWS Redshift) and build ETL pipeline for this analysis. 

### Project Description

For this project, we will build a data warehouse on AWS and build an ETL pipeline for a database hosted on Redshift.  The data is loaded from S3 buckets to staging tables on Redshift and modeled into fact and dimensions tables to perform analytics and obtain meaningful insights.

### Built With

* python
* AWS

### Dataset

#### Song Dataset

Songs dataset is a subset of [Million Song Dataset](http://millionsongdataset.com/). Each file in the dataset is in JSON format and contains meta-data about a song and the artist of that song. The dataset is hosted at S3 bucket `s3://udacity-dend/song_data`.

#### Log Dataset

Logs dataset is generated by [Event Simulator](https://github.com/Interana/eventsim). These log files in JSON format simulate activity logs from a music streaming application based on specified configurations. The dataset is hosted at S3 bucket `s3://udacity-dend/log_data`.

## Project structure

Files in this repository:

|     File / Folder      |                         Description                          |
| :--------------------: | :----------------------------------------------------------: |
||     sql_queries.py    |Contains the SQL queries for staging, schema definition and ETL|
|    create_tables.py    | Drops and creates tables on AWS Redshift (Reset the tables)  |
|         etl.py         | Stages and transforms the data from S3 buckets and loads them into tables |
|        dwh.cfg         |              Sample configuration file for AWS               |
|         README         |                         Readme file                          |


### Prerequisites

These are the prerequisites to run the program.

* python 3.7
* PostgreSQL
* AWS account
* psycopg2 python library
* boto3 python library

### How to run

Follow the steps to extract and load the data into the data model.

1. Navigate to `Project 3 Data Warehouse on AWS` folder

2. Edit the `dwh.cfg` configuration file and fill in the AWS Access Key and Secret Key fields

3. Run `create_tables.py` to create/reset the tables by

   ```python
   python create_tables.py
   ```

5. Run ETL process and load data into database by 

   ```python
   python etl.py
   ```

   This will execute SQL queries corresponding to staging data from S3 on Redshift and to transform and insert into the Postgres tables on Redshift.

6. Remember to clean up resource once completed.